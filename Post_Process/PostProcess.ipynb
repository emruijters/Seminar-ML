{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360\n",
    "print(\"AIF360 is working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# AIF360 core components\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"predictions_binary.csv\")\n",
    "df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"predictions_scores.csv\")\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = [\"regression_beat\",\"regression_grf_noZ\",\"regression_grf_all\",\n",
    "     \"regression_xgboost\",\"regression_beat_log\",\"regression_logistic\"] \n",
    "\n",
    "#Making sure the probabilities reflect chance of the favourable outcome which is in our case non-recidivism\n",
    "df_2[score_columns] = 1 - df_2[score_columns]\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1['age_cat_Greater than 45'] == 1][['true_value', 'regression_logistic']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1['sex_Male'] == 1][['true_value', 'regression_logistic']].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1['race_African-American'] == 1][['true_value', 'regression_logistic']].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [\"regression_beat\", \"regression_grf_all\",\"regression_beat_log\",\n",
    "                   \"regression_xgboost\",\"regression_logistic\",\"regression_grf_noZ\"]\n",
    "\n",
    "protected_info = {\n",
    "    \"race_African-American\": {\"unpriv\": 1, \"priv\": 0},\n",
    "    \"sex_Male\": {\"unpriv\": 1, \"priv\": 0 },\n",
    "    \"age_cat_Greater than 45\" : {\"unpriv\": 0, \"priv\": 1}\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairness_metric_table(df, prediction_cols, protected_info, true_label_col=\"true_value\"):\n",
    "    results = []\n",
    "\n",
    "    for attr, group_vals in protected_info.items():\n",
    "        unpriv_val = group_vals[\"unpriv\"]\n",
    "        priv_val = group_vals[\"priv\"]\n",
    "\n",
    "        for pred_col in prediction_cols:\n",
    "            df_base = df.copy()\n",
    "\n",
    "            # Create ground truth dataset\n",
    "            gt_dataset = BinaryLabelDataset(\n",
    "                favorable_label= 0,\n",
    "                unfavorable_label= 1,\n",
    "                df=df_base,\n",
    "                label_names=[true_label_col],\n",
    "                protected_attribute_names=[attr]\n",
    "            )\n",
    "\n",
    "            # Replace label column with prediction\n",
    "            df_base[true_label_col] = df_base[pred_col]\n",
    "\n",
    "            # Create prediction dataset\n",
    "            pred_dataset = BinaryLabelDataset(\n",
    "                favorable_label= 0,\n",
    "                unfavorable_label= 1,\n",
    "                df=df_base,\n",
    "                label_names=[true_label_col],\n",
    "                protected_attribute_names=[attr]\n",
    "            )\n",
    "\n",
    "            # Group mapping\n",
    "            unprivileged_groups = [{attr: unpriv_val}]\n",
    "            privileged_groups = [{attr: priv_val}]\n",
    "\n",
    "            # Metrics\n",
    "            metric = ClassificationMetric(\n",
    "                gt_dataset,\n",
    "                pred_dataset,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"protected_attribute\": attr,\n",
    "                \"prediction_column\": pred_col,\n",
    "                 \"version\": \"not-postprocessed\",\n",
    "                \"accuracy\": metric.accuracy(),\n",
    "                \"disparate_impact\": metric.disparate_impact(),\n",
    "                \"equal_opportunity_diff\": metric.equal_opportunity_difference(),\n",
    "                \"average_odds_diff\": metric.average_odds_difference()\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not = get_fairness_metric_table(df_1, prediction_cols, protected_info)\n",
    "df_not.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_fairness_metrics_only(\n",
    "    df,\n",
    "    prediction_cols,\n",
    "    protected_info,\n",
    "    true_label_col=\"true_value\",\n",
    "    metric_name=\"Average odds difference\",\n",
    "    metric_lb=-0.05,\n",
    "    metric_ub=0.05\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for attr, group_vals in protected_info.items():\n",
    "        unpriv_val = group_vals[\"unpriv\"]\n",
    "        priv_val = group_vals[\"priv\"]\n",
    "\n",
    "        for pred_col in prediction_cols:\n",
    "            # Ground truth dataset\n",
    "            gt_dataset = BinaryLabelDataset(\n",
    "                favorable_label= 0,\n",
    "                unfavorable_label=1,\n",
    "                df=df,\n",
    "                label_names=[true_label_col],\n",
    "                protected_attribute_names=[attr]\n",
    "            )\n",
    "\n",
    "            # Create dataset with raw scores (dummy label used)\n",
    "            df_prob = df.copy()\n",
    "            df_prob[\"true_value\"] = 0\n",
    "\n",
    "            pred_ds_probs = BinaryLabelDataset(\n",
    "                favorable_label= 0,\n",
    "                unfavorable_label= 1,\n",
    "                df=df_prob,\n",
    "                label_names=[\"true_value\"],\n",
    "                protected_attribute_names=[attr]\n",
    "            )\n",
    "\n",
    "            # Add scores\n",
    "            pred_ds_probs.scores = df[pred_col].values.reshape(-1, 1)\n",
    "\n",
    "            # Define group mapping\n",
    "            unprivileged_groups = [{attr: unpriv_val}]\n",
    "            privileged_groups = [{attr: priv_val}]\n",
    "\n",
    "            # Apply ROC\n",
    "            roc = RejectOptionClassification(\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups,\n",
    "                low_class_thresh=0.01,\n",
    "                high_class_thresh=0.99,\n",
    "                num_class_thresh=100,\n",
    "                num_ROC_margin=50,\n",
    "                metric_name=metric_name,\n",
    "                metric_ub=metric_ub,\n",
    "                metric_lb=metric_lb\n",
    "            )\n",
    "\n",
    "            roc.fit(gt_dataset, pred_ds_probs)\n",
    "            pred_ds_adj = roc.predict(pred_ds_probs)\n",
    "\n",
    "            # Evaluate post-ROC fairness\n",
    "            metric_adj = ClassificationMetric(\n",
    "                gt_dataset,\n",
    "                pred_ds_adj,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"protected_attribute\": attr,\n",
    "                \"prediction_column\": pred_col,\n",
    "                \"version\": \"postprocessed\",\n",
    "                \"accuracy\": metric_adj.accuracy(),\n",
    "                \"disparate_impact\": metric_adj.disparate_impact(),\n",
    "                \"equal_opportunity_diff\": metric_adj.equal_opportunity_difference(),\n",
    "                \"average_odds_diff\": metric_adj.average_odds_difference()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = get_roc_fairness_metrics_only(df_2, prediction_cols, protected_info)\n",
    "df_post.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_post.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not.to_csv(\"notPost.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.to_csv(\"Post.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aif360_clean)",
   "language": "python",
   "name": "aif360_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
